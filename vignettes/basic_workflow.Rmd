---
title: "The basic workflow"
output: 
  rmarkdown::html_vignette:
    df_print: kable
vignette: >
  %\VignetteIndexEntry{The basic workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

My colleagues and I follow a similar, if not the same, methodology for the
analysis of cuticular hydrocarbons (CHCs), considering the overall description
of the methods (e.g.extraction and Gas chromatography-Mass spectrometry (GC-MS)
analysis of these compounds). However, we have noticed that for the data
processing and analysis (statistics aside), we all have developed different
approaches/workflows, which are not always (entirely) reproducible. In fact,
they sometimes lead to confusion among us, regarding how exactly the data sets
of each other were/are being built. The so-called "master tables" (our final
data sets), as well as the steps to produce them, end up having structures that
very much depend on each individual, regardless of the data, the statistical
analyses that will be applied to them, or the objective of the study they were
produced for.

Furthermore, every time that we have to explain to a student how to process the
data to build up the master table and analyze the resulting data sets, the
student has to face the confusion delivered by the incompatibilities/differences
of the different steps of the processes from each of us. Besides, each of us has
to make an excessive time investment, by trying to connect the different steps
and methods, in order to explain them to the students (or anybody else).

Therefore, I decided to build up this package to facilitate the process of
analyzing this and similar types of GC data in R, thus providing a tool set and
reference for a semi-automated and reproducible workflow.

This guide offers a step-by-step explanation of such workflow, focusing on the
analysis of GC-MS data of cuticular hydrocarbons. That said, my intention here
is not to teach (nor show) how to extract CHCs, perform a GC-MS run with such
extracts, or to interpret the resulting chromatograms and mass-spectra to
assess their composition.

Assuming that you already have an idea to do this, this guide starts from the
point at which we would have already run all our samples (extracts) in the
GC-MS machine, and integrated their resulting total ion count (TIC)
chromatograms. Therefore, we have a collection of tabular files (i.e. CSV)
containing these results.

Now, let us begin by loading the `analyzeGC` package, as well as some other
package we will use along this guide, to be able to access their functions.

```{r message=FALSE, warning=FALSE}
library(analyzeGC)
library(GCalignR)
library(stringr)
library(dplyr)
library(tidyr)
```

# Import the GC-MS integration data

The very first think we will need to do is to import the CSV files with the
integration results of each sample. We need to create a list of data frames,
where each data frame corresponds to the integration results of one sample.
The rows of the data frames should correspond to the peaks within the sample,
and each data frame should have only two columns; one for the retention time,
and another for the abundance (area of the peak). The function
`analyzeGC::import_mh_data()` is designed to import the CSV files produced by
the MassHunter Qualitative Navigator proprietary software and shape it into the
correct format.

To use `import_mh_data()` we need to obtain a list with the paths to the CSV
files containing the integration results of each sample. This can be done with
`list.files()`, providing the path to the folder where the corresponding CSV
files are stored, and setting `pattern = ".CSV|.csv"`.
There are many ways to provide the path to the folder storing the CSV files to
`list.files()`, like writing it directly. My personal recommendation would
be to use the function `here::here()`, assuming that we are storing the CSV
files in a subfolder of an R project that would be used for the entire analysis.
However, to access the CSV files that are used to build the examples within
this package we will use `system.file()`, which allows to access files within
packages.

```{r}
samples_path_data <- list.files(path = system.file("extdata/gcms_integration"
                                                   , package = "analyzeGC")
                                #  Get all CSV files in the folder
                                , pattern = ".CSV|.csv"
                                , full.names = T) |>
  # Remove the paths of the standards from the lists
  str_subset('STD', negate = T)

standards_path_data <- list.files(path = system.file("extdata/gcms_integration"
                                                   , package = "analyzeGC")
                                #  Get all CSV files in the folder
                                , pattern = ".CSV|.csv"
                                , full.names = T) |>
  # Only include standards
  str_subset('STD')
```

In the code above we generated two lists, 'samples_path_data' and
'standards_path_data'. As their names indicate, the first contains the paths to
the files that correspond to the samples, and the later contains the paths to
the files corresponding to the external standards (we will elaborate on them
later). Notice that we also used `stringr::str_subset()` to decide which CSV
file paths to keep within the lists.

A code doing the same, but using `here::here()`, thus finding files within an R
project and not within a package (most likely your case), would look similar to
the code chunk below.
```{r eval=FALSE}
samples_path_data <- list.files(path = here::here("strings"
                                                  , "indicating"
                                                  , "path"
                                                  , "components"
                                                  , "below"
                                                  , "project"
                                                  , "root")
                                #  Get all CSV files in the folder
                                , pattern = ".CSV|.csv"
                                , full.names = T) |>
  # Do not include standards
  str_subset('STD', negate = T)
```

Now that we have the paths to the CSV files with the integration results, we
can use `import_mh_data()` to import the data frames they contain, and store
them within a list.

The function takes a second argument `patterns_2_delete`, which should be a
string indicating anything that should be removed from the name of the files to
obtain the samples names. I tend to add "DR_" in front of the sample when
naming the runs in the GC-MS files, so I can easily differentiate my files from
those of anybody else, stored in the machine, and the usage of "STD" as a label
is protocolary for naming the standard runs in the lab I work.
Therefore, I provide this strings as the `patterns_2_delete` argument of
`import_mh_data()`, for it to get the right name of the samples from their file
names. However, naming systems can vary, and you probably will add different
type of labels to the name of your run files, the idea is to provide the part
of the name of the file that does not correspond to the direct name of the
sample. In the case nothing should be removed from the name of the file, you
can omit the `patterns_2_delete` argument (its default value is an empty
string).

```{r warning=FALSE}
samples_data_list <- import_mh_data(samples_path_data
                                      , patterns_2_delete = "DR_")

standards_data_list <- import_mh_data(standards_path_data
                                      , patterns_2_delete = "STD")
```

With the previous code we just imported the data frames contained in the CSV
files, and stored them as tibbles within a list (`samples_data_list` or
`standards_data_list`). `import_mh_data()` automatically recognizes the columns 
within the CSV files that correspond to the retention time (RT) and abundance 
(Area) of each peak within a sample and removes any other column from them, thus
shaping the data frames in the format that is required within the `GCalignR`
package for the automatic alignment of peaks across samples. Additionally, each 
entry in the lists got named after the sample/run they belong.

We can verify this by calling any of the entries within the lists.

```{r}
samples_data_list[1:2]

standards_data_list[1:2]
```

We can further verify that both of our lists have the right format to be used
with the `GCalignR` algorithm, by using the `GCalignR::check_input()`function.

```{r}
check_input(samples_data_list)

check_input(standards_data_list)

```


# Align the data 

The `GCalignR` algorithm relies, as it is a common practice, on the RT values,
for the alignment of the peaks. This means that peaks with similar RT
values will be placed in the same row, for the data frame containing the aligned
data set, where each column will correspond to a sample. This method relies on
the fact that the RT of a compound depends on its physicochemical properties,
thus a given compound should theoretically have the same RT across samples.
However, RT values are prone to variation due to random and usually
uncontrollable factors. In consequence, the alignment of peaks across samples,
based on their RT values, should be performed by taking some considerations,
even when algorithms like that provided by `GCalignR`. 

Data from samples with very different composition, or that have been run on
different machines (even if having the same specifications, and following the
same protocol), or even using the same machine but at different times (e.g.
months of the year), might be very difficult to properly align with an automatic
procedure based on the RT values of the peaks detected within them. All these
cases are not precisely uncommon in chemical ecology. Your intention might be 
to compare the composition of very different samples (e.g. CHC extracts of
different insect species, castes, or populations), or you could have a very 
large sample size (thus needing to either use more than one machine, or run 
your samples along a big time span). You will probably end up having to 
manually correct some of the alignments produced by the algorithm.

Therefore, here I propose some modifications, to the workflow suggested by the 
`GCalignR` package, that aim to extend the reproducibility provided by coding
to further steps of the preparation of GC data in the context of chemical 
ecology. 

The first thing we can consider is to perform an automatic RT driven alignment
among samples that are more similar. They could be samples that were run in the 
same machine, at very close times, and that belong to the same group within our
analysis. To further clarify this point, let us consider the data set included
in this package. We just imported 20 data frames (contained in
`samples_data_list`), each of which corresponds to the integration results of 
the GC-MS data from the CHC extract of a honeybee (_Apis mellifera mellifera_)
worker. All these  were run in the same machine, one after the other, within 
two consecutive days. So, in principle there should be very low random 
variation among them, and to align them altogether might actually be a very
valid approach. However, these bees represent two different task-performance
groups. The bees were sampled at the end of the winter, in February (2020),
distinguishing between in-hive and out-hive workers, with the intention to
compare the CHC composition of both groups. 

As the name indicates, in-hive workers were bees collected directly from the
inside of the hives. Meanwhile, the out-hive workers were flying bees (possibly
foragers) returning to their hive, collected at the entrance of the respective
hive. Thus, another possible approach, and the one suggested here, would be to
perform an initial automatic RT-wise alignment for the samples of each of these
groups by separate, and merge both alignments results at a later step. This way,
we reduce the  variability between the initially aligned samples, thus reducing
the number corrections to be performed over that initial alignment.

The `mg_list()`, which is intended to be an abbreviation for "many groups list",
allows us to nest the data frames within a list in sub-lists representing the
different groups contained in our data set. To use this function, we need 
another data frame that contains the information regarding which samples belong 
to which group, which something that you most probably have already built as 
part of your sampling process. The package includes such a data frame under the
name of `grouping_info`.

```{r}
grouping_info
```

As it was mentioned before, the task (in- or out-hive workers) is the only 
factor that differentiates the samples within the data set. Thus, it makes sense
to use the factor task to nest the data frames within the `samples_data_list`
object. Nevertheless, to illustrate the option of nesting samples from groups
resulting from a more complex experimental design, with nested factors, here we
will do it for the three factors (Season, task, and subspecies). This will not
affect the final grouping of data frames, for our data set, as again, the only
factor differentiating the samples is task, so as long as we use this factor, 
it does not matter how many of the other factors we use to group them together.

To do this, we need to create a new factor, within the `grouping_info` data
frame, that indicates the group of the samples considering the combination of
all the factors of interest (Season, task, and subspecies). This process can be
achieved with the following pipe line.

```{r}
grouping_info <- grouping_info |>
  unite(group_label
        , where(is.factor)
        , sep = "_"
        , remove = F)
grouping_info
```

Now we can use `mg_list()` to nest the samples within `samples_data_list` in
sub-lists defined by the `group_label` column within `grouping_info`.

```{r}
samples_data_list <- mg_list(sample.info = grouping_info
                             , group.label = "group_label"
                             , samples.data.list = samples_data_list)

summary(samples_data_list)
```
















