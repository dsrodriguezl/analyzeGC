---
title: "The basic workflow"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{The basic workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE
  , comment = "#>"
)
```

# Introduction

My colleagues and I follow a similar, if not the same, methodology for the
analysis of cuticular hydrocarbons (CHCs), considering the overall description
of the methods (e.g.extraction and Gas chromatography-Mass spectrometry (GC-MS)
analysis of these compounds). However, we have noticed that for the data
processing and analysis (statistics aside), we all have developed different
approaches/workflows, which are not always (entirely) reproducible. In fact,
they sometimes lead to confusion among us, regarding how exactly the data sets
of each other were/are being built. The so-called "master tables" (our final
data sets), as well as the steps to produce them, end up having structures that
very much depend on each individual, regardless of the data, the statistical
analyses that will be applied to them, or the objective of the study they were
produced for.

Furthermore, every time that we have to explain to a student how to process the
data to build up the master table and analyze the resulting data sets, the
student has to face the confusion delivered by the incompatibilities/differences
of the different steps of the processes from each of us. Besides, each of us has
to make an excessive time investment, by trying to connect the different steps
and methods, in order to explain them to the students (or anybody else).

Therefore, I decided to build up this package to facilitate the process of
analyzing this and similar types of GC data in R, thus providing a tool set and
reference for a semi-automated and reproducible workflow.

This guide offers a step-by-step explanation of such workflow, focusing on the
analysis of GC-MS data of cuticular hydrocarbons. That said, my intention here
is not to teach (nor show) how to extract CHCs, perform a GC-MS run with such
extracts, or to interpret the resulting chromatograms and mass-spectra to
assess their composition.

Assuming that you already have an idea on how to do this, this guide starts from the
point at which we would have already run all our samples (extracts) in the
GC-MS machine, and integrated their resulting total ion count (TIC)
chromatograms. Therefore, we have a collection of tabular files (i.e. CSV)
containing these results.

Now, let us begin by loading the `analyzeGC` package, as well as some other
packages we will use along this guide, to be able to access their functions.

```{r message=FALSE, warning=FALSE}
library(analyzeGC)
library(GCalignR)
library(stringr)
library(dplyr)
library(tidyr)
```

# Import the GC-MS integration data

The very first think we will need to do is to import the CSV files with the
integration results of each sample. We need to create a list of data frames,
where each data frame corresponds to the integration results of one sample.
The rows of the data frames should correspond to the peaks within the sample,
and each data frame should have only two columns; one for the retention time,
and another for the abundance (area of the peak). The function
`analyzeGC::import_mh_data()` is designed to import the CSV files produced by
the MassHunter Qualitative Navigator proprietary software and shape it into the
correct format.

To use `import_mh_data()` we need to obtain a list with the paths to the CSV
files containing the integration results of each sample. This can be done with
`list.files()`, providing the path to the folder where the corresponding CSV
files are stored, and setting `pattern = ".CSV|.csv"`.
There are many ways to provide the path to the folder storing the CSV files to
`list.files()`, like writing it directly. My personal recommendation would
be to use the function `here::here()`, assuming that we are storing the CSV
files in a subfolder of an R project that would be used for the entire analysis.
However, to access the CSV files that are used to build the examples within
this package we will use `system.file()`, which allows to access files within
packages.

```{r}
samples_path_data <- list.files(path = system.file("extdata/gcms_integration"
                                                   , package = "analyzeGC")
                                #  Get all CSV files in the folder
                                , pattern = ".CSV|.csv"
                                , full.names = T) |>
  # Remove the paths of the standards from the lists
  str_subset('STD', negate = T)

standards_path_data <- list.files(path = system.file("extdata/gcms_integration"
                                                   , package = "analyzeGC")
                                #  Get all CSV files in the folder
                                , pattern = ".CSV|.csv"
                                , full.names = T) |>
  # Only include standards
  str_subset('STD')
```

In the code above we generated two lists, `samples_path_data` and `standards_path_data`. As their names indicate, the first contains the paths to
the files that correspond to the samples, and the later contains the paths to
the files corresponding to the external standards (we will elaborate on them
later). Notice that we also used `stringr::str_subset()` to decide which CSV
file paths to keep within the lists.

A code doing the same, but using `here::here()`, thus finding files within an R
project and not within a package (most likely your case), would look similar to
the code chunk below.
```{r eval=FALSE}
samples_path_data <- list.files(path = here::here("strings"
                                                  , "indicating"
                                                  , "path"
                                                  , "components"
                                                  , "below"
                                                  , "project"
                                                  , "root")
                                #  Get all CSV files in the folder
                                , pattern = ".CSV|.csv"
                                , full.names = T) |>
  # Do not include standards
  str_subset('STD', negate = T)
```

Now that we have the paths to the CSV files with the integration results, we
can use `import_mh_data()` to import the data frames they contain, and store
them within a list.

The function takes a second argument `patterns_2_delete`, which should be a
string indicating anything that should be removed from the name of the files to
obtain the samples' names. I tend to add "DR_" in front of the sample when
naming the runs in the GC-MS files, so I can easily differentiate my files from
those of anybody else, stored in the machine, and the usage of "STD" as a label
is protocolary for naming the standard runs in the lab I work.
Therefore, I provide this strings as the `patterns_2_delete` argument of
`import_mh_data()`, for it to get the right name of the samples from their file
names. However, naming systems can vary, and you probably will add different
type of labels to the name of your run files, the idea is to provide the part
of the name of the file that does not correspond to the direct name of the
sample. In the case nothing should be removed from the name of the file, you
can omit the `patterns_2_delete` argument (its default value is an empty
string).

```{r warning=FALSE}
samples_data_list <- import_mh_data(samples_path_data
                                      , patterns_2_delete = "DR_")

standards_data_list <- import_mh_data(standards_path_data
                                      , patterns_2_delete = "STD")
```

With the previous code we just imported the data frames contained in the CSV
files, and stored them as tibbles within a list (`samples_data_list` or
`standards_data_list`). `import_mh_data()` automatically recognizes the columns 
within the CSV files that correspond to the retention time (RT) and abundance 
(Area) of each peak within a sample and removes any other column from them, thus
shaping the data frames in the format that is required within the `GCalignR`
package for the automatic alignment of peaks across samples. Additionally, each 
entry in the lists got named after the sample/run they belong.

We can verify this by calling any of the entries within the lists.

```{r}
samples_data_list[1:2]

standards_data_list[1:2]
```

We can further verify that both of our lists have the right format to be used
with the `GCalignR` algorithm, by using the `GCalignR::check_input()` function.

```{r}
check_input(samples_data_list)

check_input(standards_data_list)

```

# Align the data 

The `GCalignR` algorithm relies, as it is a common practice, on the RT values,
for the alignment of the peaks. This means that peaks with similar RT
values will be placed in the same row, for the data frame containing the aligned
data set, where each column will correspond to a sample. This method relies on
the fact that the RT of a compound depends on its physicochemical properties,
thus a given compound should theoretically have the same RT across samples.
However, RT values are prone to variation due to random and usually
uncontrollable factors. In consequence, the alignment of peaks across samples,
based on their RT values, should be performed by taking some considerations,
even when using algorithms like that provided by `GCalignR`. 

Data from samples with very different composition, or that have been run on
different machines (even if having the same specifications, and following the
same protocol), or even using the same machine but at different times (e.g.
months of the year), might be very difficult to properly align with an automatic
procedure based on the RT values of the peaks detected within them. All these
cases are not precisely uncommon in chemical ecology. Your intention might be 
to compare the composition of very different samples (e.g. CHC extracts of
different insect species, castes, or populations), or you could have a very 
large sample size (thus needing to either use more than one machine, or run 
your samples along a big time span). So, you will probably end up having to 
manually correct some of the alignments produced by the algorithm.

Therefore, here I propose some modifications to the workflow suggested by the 
`GCalignR` package, aiming to extend the reproducibility provided by coding
to further steps of the preparation of GC data in the context of chemical 
ecology. 

## Group your samples

The first thing we can consider is to perform an automatic RT driven alignment
among samples that are more similar. They could be samples that were run in the 
same machine, at very close times, and that belong to the same group within our
analysis. To further clarify this point, let us consider the data set included
in this package. We just imported 20 data frames (contained in
`samples_data_list`), each of which corresponds to the integration results of 
the GC-MS data from the CHC extract of a honeybee (_Apis mellifera mellifera_)
worker. All these  were run in the same machine, one after the other, within 
two consecutive days. So, in principle there should be very low random 
variation among them, and to align them altogether might actually be a very
valid approach. However, these bees represent two different task-performance
groups. The bees were sampled at the end of the winter, in February (2020),
distinguishing between in-hive and out-hive workers, with the intention to
compare the CHC composition of both groups. 

As the name indicates, in-hive workers were bees collected directly from the
inside of the hives. Meanwhile, the out-hive workers were flying bees (possibly
foragers) returning to their hive, collected at the entrance of their respective
hive. Thus, another possible approach, and the one suggested here, would be to
perform an initial automatic RT-wise alignment for the samples of each of these
groups by separate, and merge both alignment results at a later step. 
The objective is to minimize the variability between the initially aligned
samples, thus reducing the number of corrections to be performed over that
initial alignment.

The `mg_list()` function, the name is intended to be an abbreviation for 
"many groups list", allows us to nest the data frames within a list in 
sub-lists representing the different groups contained in our data set. To use
this function, we need another data frame that contains the information
regarding which samples belong to which group; you probably have already built such a table as part of your sampling process. The package
includes such a data frame under the name of `grouping_info`.

```{r}
grouping_info
```

As it was mentioned before, the task (in- or out-hive workers) is the only 
factor that differentiates the samples within the data set. Thus, it makes sense
to use the factor task to nest the data frames within the `samples_data_list`
object. Nevertheless, to illustrate the option of nesting samples from groups
resulting from a more complex experimental design, with nested factors, here we
will do it for the combination of the three factors (Season, task, and
subspecies). This will not affect the final grouping of data frames, for our
data set, as again, the only factor differentiating the samples is task, so as
long as we use this factor, it does not matter how many of the other factors we
use to group them together, they will only differentiate within the task factor.

To do this, we need to create a new factor, within the `grouping_info` data
frame, that indicates the group of the samples considering the combination of
all the factors of interest (Season, task, and subspecies). This process can be
achieved with the following pipe line.

```{r}
grouping_info <- grouping_info |>
  unite(group_label
        , where(is.factor)
        , sep = "_"
        , remove = F)
grouping_info
```

Now we can use `mg_list()` to nest the samples within `samples_data_list` in
sub-lists defined by the `group_label` column within `grouping_info`.

```{r}
samples_data_list <- mg_list(sample.info = grouping_info
                             , group.label = "group_label"
                             , samples.data.list = samples_data_list)

summary(samples_data_list)
```

The new version of `samples_data_list` is a list of two lists, containing 10
data frames each, where every sub-list represents one of the groups defined
within the `group_label` column in `grouping_info`. Now, we can apply functions
to each of these lists separately, using `lapply`. For example, we could apply
`GCalignR::check_input()` to verify that each of them can be aligned with the
`GCalignR` algorithm.

```{r}
samples_data_list |> 
  lapply(check_input)
```

## What about the standards? 
The standards are commercial artificial mixtures of n-alkanes, in which each
compound is in a 40mg/l concentration. In our lab we use two different mixtures,
which correspond to the two data frames in `standards_data_list` (H2005, and
L2005). L2005 corresponds to a mixture of the n-alkanes C8 to C20, and H2005
corresponds to a mixture of the n-alkanes C21 to C40. The "L" and "H" in their
names state for "low"and "high" given their relative RT values, as the longer
n-alkanes have higher RT values than the shorter ones. The "2005" in their name,
indicates the Year (2020) and month (May)  when they were run in the GC-MS (we
run them at least once a month in the lab). The point of regularly running the
standards, is that they give very important information regarding the status of
the machine, within a given time window. This is the reason why we include them
in our analysis workflow, and they will be used for two very specific procedures
within this guide. But we will discuss them in detail later, first we need to
perform our alignments.

This package includes the `align_chromatograms2()` function, which is a wrapper 
around `GCalignR::align_chromatograms()`, so you should credit them as well if 
you use `align_chromatograms2()`. 
Unlike `GCalignR::align_chromatograms()`, `align_chromatograms2()` uses an internal seed for the selection of the reference sample for the alignment (This is done via `withr::local_seed()`, so do not worry, the execution of the function does not change the status of the random number generator outside of the function's environment). 
This guarantees that using a given set of parameters for a given data set, you will always get the same alignment result.

## The alignment parameters

`align_chromatograms2()` requires the same parameters as
`GCalignR::align_chromatograms()` to perform the alignment of your samples,
which are very well explained in the original `GCalignR`
[paper](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0198311), so please go and check it. 
The difference here, is that I changed the name of the function parameters to more closely reflect the description of the corresponding alignment step in their paper (e.g. `max_diff_peak2mean` is now called `partial_alignment_threshold`).


But how do we determine the values we can give to this parameters?

### The linear shift
For the linear shift, the default value in `GCalignR::align_chromatograms()` 
is 0.02, but `align_chromatograms2()` has no default for the corresponding
parameter (`linear_shift_criteria`) to force you to be explicit with your
alignment criteria, as this is usually a very arbitrary criteria. My advice
would be to keep it as a very small value (a very conservative approach would
be to just set it as 0), on the end you should not be aligning samples that
have a very strong RT variability for a given compound (at least, under the
considerations of the workflow presented here). Here we will just take the
original default value of 0.02, as it seemed to work well for this particular 
data set during my initial trials of the function. Some possible approaches to
define a linear shift that is more bounded to the specific data set, thus less
arbitrary, would be to have standard runs before and after running a set of
samples and use the average difference in RT between homologous peaks. This
implies that you think in advance the number of samples you can run between
standard runs, without separating them too much in time. Moerover, it might not 
be possible to use this approach, if you want to analyze GC-MS runs from the
past (re-running the samples might also not be a viable option in some cases).
An alternative approach, if you used an internal standard for absolute
quantification of the abundance of the compounds in your samples, would be to
use the average RT difference of the internal standard peak across samples.
However, an internal standard is not always used (sometimes is not possible),
and once a sample has been extracted there is no use to adding the internal to
the corresponding extract. 
Therefore, it might be just easier to stick to low values; I usually just use the half/third of the `partial_alignment_threshold`.

### The partial peak alignment
In the case of the partial peak alignment, a very recommendable approach would
be to use `GCalignR::peak_interspace()` to find and use the minimum
separation between neighboring (adjacent) peaks within samples.
We can easily apply this function to each of the list inside `samples_data_list`
with `lapply()`.

```{r}
lapply(samples_data_list
       , peak_interspace
       , rt_col_name = "RT"
       , quantile_range = c(0, 0.8))
```

The previous plots and tables tell us that there are no peaks separated by less
than 0.04 minutes within all the samples of our data set. So, if we set this
value as the `partial_alignment_threshold`, any pair of peaks with that or
higher RT difference between samples in our data frames will be considered as
different peaks (rows). This approach could be a very easy and precise way to
set the parameter, with particular benefits for data set with lots of samples
that include lots of compounds, and my preferred approach to any realistic
research data set. However, here I will take advantage on the fact that our 
data set is not so big (20 samples), and I actually already know it very well 
(I had to analyze it in advance to be able to assess the benefits of the
workflow, and I can tell you it cut at least 30% of the time I needed to 
produce the same result, and now the process is more reproducible).

In consequence, I will set the `partial_alignment_threshold` to 0.05, as I 
could not find, during my initial analysis of the data set, any separated
hydrocarbon peaks within the samples with a separation lower than this value.
Nevertheless, we do not normally know such details of a given data set before 
we have analyze it, with data sets of bigger size such procedure could be very
time consuming and prone to errors, and it is certainly not a reproducible
method. So, I would recommend to not use such approach, and stick to the
previously described approach in any realistic research scenario. 
I am only using this approach here, because I used it for the initial 
assessment of the `GCalignR` workflow, and have confirmed that it works
properly.

### Merging rows
Finally we have to determine the threshold for the row merging step, 
`row_merging_threshold` within `align_chromatograms2()`. 
One possible approach would be to use the maximum distance between homologous
peaks between standard runs before and after our samples, or between the peaks 
of an internal standard among samples, similar to the `linear_shift_criteria`.
But, as previously said, these approaches might not always be possible.
Therefore, taking advantage of my previous knowledge on the data set we are
using here, I decided to set the `row_merging_threshold` to 0.15, which was the
maximum separation I could find between peaks of any given hydrocarbon among 
the samples. Again, this approach might only make sense (at least time-wise)
with small data sets, and or data set about which we have a good prior
knowledge, it might not be really reproducible, and may not apply to other 
types of GC data (i.e. non-CHC  extracts). 
A less time-demanding version of this approach, and the one I would normally recommend, would be to find the maximum separation between homologous peaks across samples, considering only the peaks with the highest abundance. 
As bigger peaks tend to vary more in their RT, they are a good proxy for the upper limit of variation in RT across samples for a specific substance. 

## Perform the automatic alignment
Now that we have decided the values we will use for our alignment parameters,
we can use `align_chromatograms2()` to perform the automatic alignment of our
data.
We can easily apply this function to every list within `samples_data_list`
with `lapply()`, but in the case we have a data set with several groups, it 
might take a while until we get the alignment of all the group lists within
our `samples_data_list`. Therefore, here I will show an example of code that
parallelize this alignment operations.

```{r}
# We need the doParallel package 
library(doParallel)

## Define the number of cores that will be used for the parallel processes
no_cores <- 2

## Set a processing cluster
cl <- makeCluster(no_cores)

## Register the processing cluster to be used for parallel operations
registerDoParallel(cl)

aligned_samples_data_list <- parLapply(cl
                                       , samples_data_list
                                       , align_chromatograms2
                                       , linear_shift_criteria = 0.02
                                       , partial_alignment_threshold = 0.05
                                       , row_merging_threshold = 0.15)

stopCluster(cl)
registerDoSEQ()
```

In the code above we used the `doParallel` package to register a processing
cluster, using two of our processor's cores, and then with this cluster we 
used `parLapply()` to align the lists within `samples_data_list` in parallel.
`parLapply()` works like `lapply()`, but unlike `lapply()`, it performs the
processes in parallel (not consecutively). As we use two cores for this, we
effectively cut by half the time the process would have taken if using
`lapply()`. However, be careful wen paralleling processes in R, more cores
are not always better, and you should consider the capabilities of the PC you 
are using before registering any bigger processing cluster. In general I would
recommend you to read carefully the `doParallel` package before trying to 
parallelize processes with it.

In the case of the `standards_data_list` we can just directly use
`align_chromatograms2()` on it. The standards usually have much more discrete 
RT values than the samples, so using the same alignment parameters as with the
samples would most probably give a very good alignment result (which do not 
tend to contain any miss-alignment).

```{r message=FALSE, warning=FALSE, results='hide'}
aligned_standards <- standards_data_list |> 
  align_chromatograms2(linear_shift_criteria = 0.02
                       , partial_alignment_threshold = 0.05
                       , row_merging_threshold = 0.15)
```

## Check the precision of the alignments
The goal of the automatic alignment is to minimize the number of 
miss-alignments in the aligned data frames, to reduce as much as possible the
time and effort required to manually correct them. At the same time,
it is crucial to reliably find possible miss-alignments within the data frames,
as the process of manually compare every peak across every sample in a data
frame can be very time consuming, not reproducible, and prone to errors. 

The `GCalignR` package provides the `gc_heatmap()` function, which can generate
two different types of heat maps, designed to help finding peaks that might not
be properly aligned, and are properly explained among the package documentation.
However, we have not found such plots particularly helpful to spot
miss-alignments within the aligned data frames. In the lab we came with an 
alternative heat map plot, which displays the log transformed normalized abundance of the peaks (log(%)) for every sample. 
Additionally, the samples are ordered regarding their similarity within the plot, via a hierarchical cluster that is displayed as a dendrogram. 
For us, such plot has resulted considerably more useful for finding possible miss-alignments, while it also offers a first overall view of the data within each aligned group data set. 
As a result, this package includes the function `diagnostic_heatmap()`, which produces this plot. 

`diagnostic_heatmap()` calls `area_norm()` (which is a wrapper around 
`GCalignR::norm_peaks()`) under the hood, to normalize the abundance data before
generating the plot.

We just need to indicate the function that the data comes from
`align_chromatograms2()`, by setting the `alignment.type` parameter to 
"automatic".

_Note: The diagnostic plot tends to be very big, so I recommend you to use_
`pdf()` _to generate the plots within a PDF document, where they will be easier_
_to explore. Given the illustrative purposes of this guide, I am not doing that_
_to be able show the plots here._ 
```{r eval=F, include=T}
for (df in names(aligned_samples_data_list)) {
  diagnostic_heatmap(aligned_samples_data_list[[df]]
                     , title = paste0("Alignment of "
                                      , df)
                     , alignment.type = "automatic")
}
```
```{r echo=FALSE, warning=FALSE, out.width="100%"}
knitr::include_graphics(system.file("extdata/uncorrected-alignment-IW.png", package = "analyzeGC"))

knitr::include_graphics(system.file("extdata/uncorrected-alignment-OW.png", package = "analyzeGC"))
```

In the plots, the columns represent peaks (which are labeled as P1 to Pn), 
and the rows correspond to the samples. Also, as the abundance of the peaks 
vary within a broad scale, the function transforms the normalized abundance (%)
to the log(1+x) (where x is the normalized abundance of a peak within a sample),
so peaks with low abundance are discernible in the plot.

These plots are very useful to highlight possible miss-alignments, as they
appear as breaks in the color columns of each peak, which we tend to call
“stairs” due to their appearance. Within the PDF file is easy to zoom in and
out to check the number of the peaks and samples' IDs for a cell that is causing
the stair, to then confirm if it corresponds or not to a miss-alignment.

In the case of the standards, the plot tends to be less impressive, as no short
chain-length standard is wrongly aligned with a long chain-length standard.
However, if you have several standard runs, it is recommended to pay attention
to the plot, as some miss-alignments might occur between different runs of the
same standards.
```{r eval=FALSE, include=T}
diagnostic_heatmap(aligned_standards
                   , title = "Alignment of standards"
                   , alignment.type = "automatic")
```
```{r echo=FALSE, warning=FALSE, out.width="100%"}
knitr::include_graphics(system.file("extdata/uncorrected-alignment-std.png", package = "analyzeGC"))
```

By looking at the plot, you can easily, and rapidly, spot possible
miss-alignments, and identify to which peaks and samples do the miss-aligned
values correspond. Once you know the peak label and the sample for which a peak
value could be miss-aligned, you can look after it within the RT data frame of 
the corresponding aligned data set. The package includes the `extract_RT()`
function to facilitate this, as it extracts the RT data frame from an aligned 
data set.

```{r}
RT_df_list <- aligned_samples_data_list |> 
  lapply(extract_RT)

```

After extracting the RT data frames, we can look into them for the possible
miss-alignments we identified within the diagnostic plots. One way would be to 
export the data frame tables into XLSX or CSV files, to be able to easily 
explore them. For practicity, in the example I will show you here, I will just
filter for three consecutive peaks within the in-hive workers that contain 
miss-alignments.

```{r}
RT_df_list$`Winter_In-hive workers_A. m. mellifera` |> 
  filter(row.names(RT_df_list$`Winter_In-hive workers_A. m. mellifera`) %in% 
           c("P106", "P107", "P108"))

```

As you can see, the rows of the peaks break at samples 351 and 352, for the
peaks P106 and P107 respectively. These breaks look like stairs, which is why I
call them this way. So, now that we know the RT values of the peaks where a miss-alignment could be occurring, we can check them within the chromatograms, and in the case of GC-MS data, we could even check their mass spectra. 
By such process we can properly verify if the rows forming a stair do actually
correspond to the same peak, thus being a miss-alignment.

# Correct the alignment
Once all the miss-alignments have been identified, it is necessary to correct
them. 
The correction of miss-alignments consist on the displacement of the peaks'
Area and RT values within samples, to obtain a data frame where each row
corresponds to the same peak, thus same compound(s), across samples. 
This can be done using the `add_empty_peaks()` and `correct_alignment()`
functions. 
These functions take an aligned data set to modify, and a list of
data frames, which indicate the modifications to be performed on the aligned
data set.

## Add empty peaks
If the values within a row, do not correspond to the same peak, the miss-aligned
values should be displaced to a different peak (row).
However, if the values to displace represent different peaks to those 
of the rows around it, then we need to insert an empty peak (row)
where we can place the values we need to displace.
This is not the case of our data set, but for the purpose of this guide, let's 
assume that we need to create an empty peak after P100 in the aligned data of
the in-hive workers, and before P100 in that of the out-hive workers.

`add_empty_peaks()` adds peaks with only 0s to the Area and RT data frames
within an aligned data set, provided to the function as the `aligned_data`
parameter. 
The `empty.peaks` parameter, should be a list of data frames with two columns 
(`position.reference` and `direction`). 
`position.reference` indicates a peak (e.g. P100) as a location reference
within the aligned data set. 
`direction` ("before" or "after") indicates whether the empty peak should be
created as the row before or after the reference peak.

First, we need to create the list of data frames with the instructions to create
the empty peaks. In the list, each entry name corresponds to the label of any
sample within the aligned group data to modify.
`add_empty_peaks()` will only create the empty row if the reference sample is in the data frame it is modifying.

```{r}
# Sample 335 is an In-hive worker
# Sample 339 is an Out-hive worker
empty_peaks <- list("335" = tribble(~position.reference, ~direction,
                                        "P100", "after")
                    , "339" = tribble(~position.reference, ~direction,
                                          "P100", "before"))
empty_peaks
```

We can apply `add_empty_peaks()` to each of the aligned data sets (in-/out-hive
workers) within `aligned_samples_data_list` with `lapply()`.

```{r results='hide'}
example_with_empty_peaks <- aligned_samples_data_list |>
  lapply(add_empty_peaks
         , empty.peaks = empty_peaks)
```

To confirm that the empty rows where inserted in the right place of each
group data set, we just need to look at their RT data frames.

```{r}
example_with_empty_peaks |> 
  lapply(extract_RT) |> 
  lapply(slice, 99:102)
```

As you can see, we have the new empty peaks "after_P100" and "before_P100" in 
the position we indicated, within the data frame of the right group
(In-/out-hive workers).

## Displace the peak values to correct the alignment
`correct_alignment()` allows the displacement of the Area and RT values of
peaks, within samples, to correct their alignment.
It iterates through the samples in an aligned data set (given as the
`aligned_data` parameter), displacing the RT and Area values within the data
frames, as indicated by a list of data frames (given as the `movements_list`
parameter). The entry names in the list provided as `movements_list` should 
correspond to the sample label within the data, for which the modifications
encoded in the corresponding data frame within the list should be applied.
These data frames must have two columns (`peaks_list` and `movement_dirs`), 
indicating the peaks whose RT and Area values should be displaced (e.g. P100),
and in what direction ('up' or 'down'), for a given sample.

To be able to use `correct_alignment()`, we first need to create a list of 
data frames, indicating how to displace the peak RT and Area values of each 
sample in the data set. We should consider that `correct_alignment()` displaces
the peak values one after the other, in the order they are listed within a data 
frame. Hence, we should set the moving instructions in a way that the values of
one peak do not overwrite the values of another, or we will be losing data.
For example, if the values of two neighboring peaks should be displaced to the
upper row, then the values of the first peak should be displaced before the
other, or the values of the latter peak will overwrite the values of the first.

```{r}
peaks_movements <- list("350" = data.frame(peaks_list = c(paste0("P"
                                                                 , c(108
                                                                     , 126)))
                                           , movement_dirs = c('up', 'up'))
                        , "351" = data.frame(peaks_list = c(paste0("P"
                                                                   , c(107
                                                                       , 108
                                                                       , 119
                                                                       , 120
                                                                       , 126)))
                                             , movement_dirs = c('up'
                                                                 ,'up'
                                                                 , 'up'
                                                                 , 'up'
                                                                 , 'up'))
                        , "352" = data.frame(peaks_list = c(paste0("P"
                                                                   , c(107
                                                                       , 108
                                                                       , 126)))
                                             , movement_dirs = c('up'
                                                                 ,'up'
                                                                 , 'up'))
                        , "333" = data.frame(peaks_list = c(paste0("P"
                                                                   , c(26)))
                                             , movement_dirs = c('down'))
                        , "345" = data.frame(peaks_list = c(paste0("P"
                                                                   , c(106
                                                                       , 107)))
                                             , movement_dirs = c('up', 'up'))
                        )

peaks_movements
```

Now we can apply `correct_alignment()` to each of the group data sets in 
`aligned_samples_data_list`, using `lapply())`. 
If we would only have a single aligned data set (like in the case of the
standards), instead of a list of them, `correct_alignment()` could be directly
use on it without `lapply()`

```{r results='hide'}
corrected_samples_list <- lapply(aligned_samples_data_list
                                 , correct_alignment
                                 , movements_list = peaks_movements)
```

We can check that all the miss-alignments were corrected by using  
`diagnostic_heatmap()`. 
However, as the objects returned by `align_chromatograms2()` and
`correct_alignment()` have different structures, we need to specify that the
data corresponds to a corrected alignment by setting the parameter
`alignment.type` to "corrected".

```{r eval=F, include=T}
for (df in names(samples_area_norm_list)) {
  diagnostic_heatmap(samples_area_norm_list[[df]]
                     , title = paste0("corrected alignment of "
                                      , df)
                     , alignment.type = "corrected")
}
```
```{r echo=FALSE, warning=FALSE, out.width="100%"}
knitr::include_graphics(system.file("extdata/corrected-alignment-IW.png", package = "analyzeGC"))

knitr::include_graphics(system.file("extdata/corrected-alignment-OW.png", package = "analyzeGC"))
```

## Recalculate the mean RT
Along the alignment correction process, the column holding the mean RT is 
removed from the aligned data sets, which would no longer be correct due to the
displacement of the peak values. 
We need, in consequence, to recalculate the mean RT and add it to the data sets,
for which we can use `recalculate_meanRT()`.

`recalculate_meanRT()` can be directly used on a corrected alignment obtained
with `correct_alignment()`, but in the case of a list of them , like
`corrected_samples_list`, we need to use `lapply()` to apply the function to 
every data set contained in the list.

```{r}
corrected_samples_list2 <- lapply(corrected_samples_list
                                  , recalculate_meanRT)
```

`recalculate_meanRT()` does not only recalculate and add the mean RT to the 
RT and Area data frames. 
It also erases any empty row that was created during the alignment correction
process, transform 0s into NAs within the RT data frames, and reorder the
samples within the data frames. 
In the new data frames, the samples are in descending order, regarding the sum
of the abundances of the peaks they contain. 

```{r}
corrected_samples_list2 |> 
  lapply(extract_RT)
```


We have now the corrected alignment of each group, but we do not know what 
compounds are contained within each peak. 
At this point, I would extract the RT data frames of each group, and the
standards, using `extract_RT()` and export them as CSV or XLSX files, where they
will be easy to explore.
The point is to find the peaks of the different samples, within the data frames,
in their chromatograms, and identify the hydrocarbon(s) they contain, by looking
at their mass-spectra.
Here, is where it might result helpful the new order of the samples within the
data frames. 
If a sample has a higher total abundance, it means it has more peaks than the others.
Therefore, if we begin exploring the chromatogram of the first sample of a 
given data frame, we will be able to identify most of the compounds present in
the other samples of the data frame.
Then, when exploring the last samples of the data frame, we will have a good 
idea of which peaks do not contain hydrocarbons (or other compound(s) of our
interest), thus we will mainly need to confirm the identification of the
compounds among the samples in the data frame, and identifying those that were
absent in most of the samples.

At the end of the compounds' identification process, we will have a new table 
per data frame (a modified version of the exported RT data frame), that
will contain the identification of the compounds within the different peaks.
The data frames contained in these new tables could then be loaded in R for the
next step of the workflow (the preparation of the group tables).
The resulting data frames for the data used in this guide are contained in the 
package as `comps_id_std` and `comps_id_list`.

```{r}
lapply(comps_id_list, str, give.attr = F)

str(comps_id_std, give.attr = F)
```

As you can see, these data frames are very similar to the original RT data 
frames, except for their second column, which is called "Compound" and holds 
the identification of the compounds contained by each peak. 
This is because, when creating these tables, I simply added that column to the
exported RT data frames. 
However, the only important columns for these data frames are "Peak" and
"Compound", most functions of the package will work as long as they are present.

# Prepare the group tables

Now that we have identified the compounds contained by the peaks within our 
samples, for the data of each group, we need to assemble the group tables.
I am assuming we have already loaded the data frames with the compound
information of the samples and the standards, as they come within the package, 
and you might have your own preference on how you want to produce (thus import)
such data frames for your own data.

## Shape the data frames
To prepare the group tables, we first need to put together the new data frames 
that contain the compound identification, and perform some data wrangling
operations to shape the group tables.

### The standards

```{r dpi = 65, fig.width=11, fig.height=8}
std_info <- shape_hcstd_info(comps_id.STD = comps_id_std
                             , aligned_std = aligned_standards
                             , short_std_pattern = "L"
                             , long_std_pattern = "H")
```


### The samples


```{r}
comps_info_list <- comps_id_list |>
  lapply(get_chc_info)

comps_info_list |> str()
```


system.file("extdata/gcms_integration", package = "analyzeGC")
```{r eval=F, include=T}
adjusted_samples_list <- corrected_samples_list2 |>
  lapply(adjust_abundance, std.info = std_info)
```
```{r echo=FALSE, warning=FALSE, out.height=300, out.width=500}
knitr::include_graphics(system.file("extdata/samples_correction-plots.pdf"
                                    , package = "analyzeGC"))
```


```{r}
unfiltered_samples_list <- add_comps_info(samples.list = adjusted_samples_list
                                          ,comps.info.list = comps_info_list)
unfiltered_samples_list |> lapply(str, give.attr = F)
```

## Clean the data

### Remove trace compounds

```{r results='hide'}
filtered_samples_list <- unfiltered_samples_list |>
  lapply(trace_comps
         , threshold = 0.01)
```

### Remove non-target compounds

```{r}
filtered_samples_list2 <- filtered_samples_list |>
  lapply(drop_na_compounds)
```

## Calculate the Kováts (retention) index

```{r results='hide', message=FALSE, warning=FALSE}
samples_plus_ri_list <- filtered_samples_list2 |>
  lapply(kovats_retention_index, std.info = std_info)
```

## Shape the group tables

```{r}
group_tables_list <- samples_plus_ri_list |>
  lapply(shape_group_table)

group_tables_list
```

# The master table

## Build the first master table

```{r}
master_table <- build_master_table(group_tables_list)

master_table
```

## Fusing peaks

### Find duplicated compounds

```{r}
duplicated_compounds_presence <-
  retrieve_group_tables(group.label = "group_label"
                        , master.table = master_table
                        , grouping.info = grouping_info) |>
  assess_duplicated_compounds(plot = F)

duplicated_compounds_presence
```

### Fuse peaks

```{r}
fusion_list <- list(c(paste0("P"
                             , c(1, 2)))
                    , c(paste0("P"
                               , c(7, 8)))
                    , c(paste0("P"
                               , c(9, 10)))
                    , c(paste0("P"
                               , c(13, 14)))
                    , c(paste0("P"
                               , c(15, 16)))
                    , c(paste0("P"
                               , c(19, 20)))
                    , c(paste0("P"
                               , c(24, 25)))
                    , c(paste0("P"
                               , c(27, 28)))
                    , c(paste0("P"
                               , c(29, 30)))
                    , c(paste0("P"
                               , c(32, 33)))
                    , c(paste0("P"
                               , c(38, 39)))
                    , c(paste0("P"
                               , c(40, 41)))
                    , c(paste0("P"
                               , c(42, 43)))
                    , c(paste0("P"
                               , c(44, 45)))
                    , c(paste0("P"
                               , c(46, 47)))
                    , c(paste0("P"
                               , c(49, 50)))
                    , c(paste0("P"
                               , c(52, 53)))
                    , c(paste0("P"
                               , c(58, 59)))
                    , c(paste0("P"
                               , c(60, 61)))
                    , c(paste0("P"
                               , c(62, 63)))
                    , c(paste0("P"
                               , c(66, 67)))
                    , c(paste0("P"
                               , c(69, 70)))
                    , c(paste0("P"
                               , c(72, 73)))
                    , c(paste0("P"
                               , c(74, 75)))
                    , c(paste0("P"
                               , c(76, 77, 78)))
                    , c(paste0("P"
                               , c(80, 81)))
                    , c(paste0("P"
                               , c(82, 83)))
                    , c(paste0("P"
                               , c(84, 85)))
                    , c(paste0("P"
                               , c(86, 87)))
                    , c(paste0("P"
                               , c(88, 89))))
```

```{r results='hide'}
master_table2 <- fuse_all_peaks(master.table = master_table
                                , fusion.list = fusion_list)
```
```{r}
master_table2
```

## Filter out rare compounds (optional)

```{r}
grouping_info <- grouping_info |>
  unite(group_label
        , where(is.factor)
        , sep = "_"
        , remove = FALSE)
```


```{r results='hide'}
group_tables_list2 <- retrieve_group_tables(group.label = "group_label"
                                            , master.table = master_table2
                                            , grouping.info = grouping_info) |>
  lapply(group_frequency_filter)
```
```{r}
group_tables_list2
```

##  Re-build the master table

```{r}
master_table_reassembled <- build_master_table(group_tables_list2)

master_table_reassembled
```

## Transform the abundance

```{r}
master_table_final <- abundance_transformation(master_table_reassembled)
master_table_final
```

## Get the final data frames for analysis

```{r}
grouping_info <- grouping_info |> 
  mutate(Individual = as.factor(Individual)
         , group_label = NULL)

str(grouping_info)
```

```{r}
master_comps <-  master_table_final |> 
  select(Peak:Mod.position)

str(master_comps)
```

```{r}
master_daten <- get_daten(master_table_final)

str(master_daten)
```


# Analyze the data

```{r}
library(ggplot2)
```

```{r fig.width=8.5, fig.height=9}
grouping_info |> 
  bind_cols(master_daten) |> 
  pivot_longer(cols = !where(is.factor)
               , names_to = "Peak"
               , values_to = "Abundance") |> 
  mutate(Peak = factor(Peak, levels = unique(Peak))) |> 
  ggplot(aes(x = Individual, y = Peak)) +
  geom_raster(aes(fill = log1p(Abundance))) +
  scale_fill_viridis_c(option = "turbo")  +
  # Y axis is sorted from shorter chain length to longer chain length
  # labels correspond to the compound name
  scale_y_discrete(limits = rev
                   , labels = master_comps |>
                     arrange(desc(RI)) |>
                     pull(Compound)) +
  # Produce facets wrapping the plot regarding the task groups
  facet_wrap(vars(Task), scales = "free_x") +
  theme_classic()
  
```

```{r}
library(vegan)
```

```{r results = "hide"}
set.seed(12345)
sol <- metaMDS(master_daten
               , distance = "bray"
               , try = 200
               , k = 2
               , trymax = 400)
```

```{r}
nmds_data <- grouping_info |> 
  bind_cols(scores(sol, "sites"))

nmds_data
```


```{r fig.width=7, fig.height=4}
nmdsPlot <- ggplot(data = nmds_data
                   ,aes(x = NMDS1
                        , y = NMDS2)) +
  geom_point(aes(shape = Task
                 , fill = Task)
             , alpha = 0.75
             , size =2) +
  scale_fill_viridis_d() +
  scale_shape_manual(values = c("In-hive workers" = 25
                                , "Out-hive workers" = 23)) +
  coord_equal() +
  theme_classic()
nmdsPlot
```



